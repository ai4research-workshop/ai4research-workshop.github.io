,,,,,,
AAAI26_W10_1,Beyond Chemical QA: Evaluating LLM’s Chemical  Reasoning with Modular Chemical Operations,"While large language models (LLMs) with Chain-of-Thought (CoT) reasoning excel in mathematics and coding, their potential for systematic reasoning in chemistry, a domain demanding rigorous structural analysis for real-world tasks like drug design and reaction engineering, remains untapped. Current benchmarks focus on simple knowledge retrieval, neglecting step-by-step reasoning required for complex tasks such as molecular optimization and reaction prediction. To address this, we introduce ChemCoTBench, a reasoning framework that bridges molecular structure understanding with arithmetic-inspired operations, including addition, deletion, and substitution, to formalize chemical problem-solving into transparent, step-by-step workflows. By treating molecular transformations as modular ""chemical operations"", the framework enables slow-thinking reasoning, mirroring the logic of mathematical proofs while grounding solutions in real-world chemical constraints. We evaluate models on two high-impact tasks: Molecular Property Optimization and Chemical Reaction Prediction. These tasks mirror real-world challenges while providing structured evaluability. By providing annotated datasets, a reasoning taxonomy, and baseline evaluations, ChemCoTBench bridges the gap between abstract reasoning methods and practical chemical discovery, establishing a foundation for advancing LLMs as tools for AI-driven scientific innovation.",Li Hao; He CAO; Bin Feng; Daniel Shao; Xiangru Tang; Zhiyuan Yan; Yonghong Tian; Li Yuan; Yu Li,2101212812@pku.edu.cn; hcaoaf@connect.ust.hk; fengbin@idea.edu.cn; yanjun.shao@yale.edu; xiangru.tang@yale.edu; yanzhiyuan1114@gmail.com; yhtian@pku.edu.cn; yuanli-ece@pku.edu.cn; liyu@idea.edu.cn,Oral,Oral 1,
AAAI26_W10_2,"The Trajectory of Graph of Thoughts: Advancing AI for Science through Structured, Verifiable, and Causal Reasoning","The advancement of ""AI for Science"" requires cognitive architectures capable of navigating the inherent complexities of scientific discovery—a process characterized by non-linear exploration, iterative refinement, and the necessity for causal understanding. Traditional reasoning frameworks for Large Language Models (LLMs), such as Chain-of-Thought (CoT) and Tree-of-Thoughts (ToT), are structurally inadequate for modeling the dynamic and interconnected nature of scientific inquiry. The Graph of Thoughts (GoT) paradigm offers a crucial topological generalization, modeling reasoning as an arbitrary graph that supports synthesis, recurrence, and complex aggregation. This position paper explores the trajectory of GoT as a foundational architecture for autonomous scientific discovery agents. We analyze how recent innovations, including the integration of dynamic knowledge graphs (KGoT), adaptive inference structures (AGoT), and multi-agent collaboration graphs, enhance the capabilities of GoT. Furthermore, we focus on specialized adaptations critical for scientific integrity: High-Integrity GoT (HI-GoT) for verifiable hypothesis validation and Causal GoT (C-GoT) for modeling dynamic Structural Causal Models (SCMs). We argue that the convergence of these graph-structured reasoning frameworks provides the necessary mechanisms for developing autonomous systems capable of robust, interpretable, and causally grounded scientific discovery.",David Scott Lewis,david.lewis@aiexecutiveconsulting.com,,,
AAAI26_W10_3,Ecological AI: An Open-World Framework for Plant Taxonomic Classification,"AI-guided classification of ecological families, genera, and species underpins global sustainability efforts such as biodiversity monitoring, conservation planning, and policy-making. Progress toward this goal is hindered by long-tailed taxonomic distributions from class imbalance, along with fine-grained taxonomic variations, test-time spatiotemporal domain shifts, and closed-set assumptions that can only recognize previously seen taxa. We introduce the Open-World Ecological Taxonomy Classification, a unified framework that captures the co-occurrence of these challenges in realistic ecological settings. To address them, we propose TaxoNet, an embedding-based encoder with a dual-margin penalization loss that strengthens learning signals from rare underrepresented taxa while mitigating the dominance of overrepresented ones, directly confronting interrelated challenges. We evaluate our method on diverse ecological domains: Google Auto-Arborist (urban trees), iNat-Plantae (Plantae observations from various ecosystems in iNaturalist-2019), and NAFlora-Mini (a curated herbarium collection). Our model consistently outperforms baselines, particularly for rare taxa, establishing a strong foundation for open-world plant taxonomic monitoring. Our findings further show that multimodal foundation models remain constrained in plant-domain applications.",Cheng Yaw Low; Heejoon Koo; Meeyoung Cha,chengyawlow@changwon.ac.kr; heejoon.koo.17@alumni.ucl.ac.uk; meeyoung.cha@gmail.com,,Poster 1,WS152
AAAI26_W10_4,Graph-Based Exploration for ARC-AGI-3 Interactive Reasoning Tasks,"We present a training-free graph-based approach for solving interactive reasoning tasks in the ARC-AGI-3 benchmark. ARC-AGI-3 comprises game-like tasks where agents must infer task mechanics through limited interactions, and adapt to increasing complexity as levels progress. Success requires forming hypotheses, testing them, and tracking discovered mechanics. The benchmark has revealed that state-of-the-art LLMs are currently incapable of solving these tasks. Our method combines frame processing with systematic state-space exploration using graph-based representations. It segments visual frames into meaningful components, prioritizes actions based on visual salience, and maintains a graph of explored states. By tracking visited states and tested actions, the method prioritizes actions that provide the shortest path to untested state-action pairs. The approach scored 3rd in the number of solved levels on the private leaderboard of ARC-AGI-3 Preview Challenge, significantly outperforming LLM-based solutions. This demonstrates the effectiveness of structured exploration and suggests that graph representations of novel tasks can guide exploration and enable progress in unknown environments where LLMs fail to understand task dynamics.",Evgenii Rudakov; Jonathan P. Shock; Benjamin Ultan Cowley,evgenii.rudakov@helsinki.fi; jon.shock@gmail.com; ben.cowley@helsinki.fi,,Poster 2,WS156
AAAI26_W10_5,Executable Claims: Turning Manuscript Statements into Verifiable Evidence Capsules at Writing Time,"LLM-assisted scientific writing accelerates manuscript preparation but risks numerical hallucinations and citation fabrication. We introduce \execlaims{}, a system that verifies scientific claims at writing-time by generating executable ``evidence capsules.'' Given manuscript text, we (1) extract claims using GPT-4o, (2) retrieve evidence via hybrid BM25 + embedding search over 14 scholarly data sources, (3) check numeric consistency with unit normalization, and (4) compile Python test scripts that verify quantitative assertions. We evaluate on 500 claims across 52 papers in ML, Biology, and Physics, achieving 68.3\% [65.1\%, 71.5\%] Groundedness@5, significantly outperforming BM25 (54.0\%, $p < 0.001$) and dense retrieval (62.5\%, $p < 0.001$). A user study with 12 researchers shows 73\% time reduction (18.3 $\to$ 4.9 min, $p < 0.001$, Cohen's $d=4.77$) and 47\% more inconsistencies detected. Counter-evidence surfacing reduces blind acceptance by 22pp ($p < 0.001$). We compare 6 LLMs, finding Claude Opus achieves highest entailment agreement ($\kappa=0.84$) while GPT-4o provides best cost-performance balance.",Aayam Bansal,aayambansal@gmail.com,,Poster 1,WS153
AAAI26_W10_6,XtraGPT: LLMs for Context-Aware and Controllable Academic Paper Revision via Human-AI Collaboration,"Despite the growing adoption of large language models (LLMs) in academic workflows, their capabilities remain limited to support high-quality scientific writing. Most existing systems are designed for general-purpose scientific text generation and fail to meet the sophisticated demands of research communication beyond surface-level polishing, such as conceptual coherence across sections. Furthermore, academic writing is inherently iterative and revision-driven, a process not well supported by direct prompting-based paradigms. To address these scenarios, we propose a human-AI collaboration framework for academic paper revision centered on criteria-guided intent alignment and context-aware modeling. To validate the framework, we curate a dataset of 7,000 research papers from top-tier venues annotated with 140,000 instruction-response pairs that reflect realistic, section-level scientific revisions. We instantiate the framework in XtraGPT, the first suite of open-source LLMs (1.5B to 14B parameters) for context-aware, instruction-guided writing assistance. Extensive experiments validate that XtraGPT significantly outperforms same-scale baselines and approaches the quality of proprietary systems. Both automated preference assessments and human evaluations confirm the effectiveness of XtraGPT in improving scientific drafts.",Nuo Chen; Andre Lin HuiKai; Jiaying Wu; Junyi Hou; Zining Zhang; Qian Wang; Xidong Wang; Bingsheng He,nuochen@comp.nus.edu.sg; andre_lin@u.nus.edu; jiayingwu@u.nus.edu; junyi.h@comp.nus.edu.sg; zhangzn710@gmail.com; qiansoc@nus.edu.sg; xidongwang1@link.cuhk.edu.cn; dcsheb@nus.edu.sg,,Poster 2,WS159
AAAI26_W10_7,Beyond Brainstorming: What Drives High-Quality Scientific Ideas? Lessons from Multi-Agent Collaboration,"While AI agents show potential in scientific ideation, most existing frameworks rely on single-agent refinement, limiting creativity due to bounded knowledge and perspective. Inspired by real-world research dynamics, this paper investigates whether structured multi-agent discussions can surpass solitary ideation. We propose a cooperative multi-agent framework for generating research proposals and systematically compare configurations including group size, leader-led versus leaderless structures, and team compositions varying in interdisciplinarity and seniority. To assess idea quality, we employ a comprehensive protocol with agent-based scoring and human review across dimensions such as novelty, strategic vision, and integration depth. Our results show that multi-agent discussions substantially outperform solitary baselines. A designated leader acts as a catalyst, transforming discussion into more integrated and visionary proposals. Notably, we find that cognitive diversity is a primary driver of quality, yet expertise is a non-negotiable prerequisite, as teams lacking a foundation of senior knowledge fail to surpass even a single competent agent.These findings offer actionable insights for designing collaborative AI ideation systems and shed light on how team structure influences creative outcomes.",Nuo Chen; Yicheng Tong; Jiaying Wu; Minh Duc Duong; Qian Wang; Zou Qingyun; Bryan Hooi; Bingsheng He,nuochen@comp.nus.edu.sg; tong828@comp.nus.edu.sg; jiayingwu@u.nus.edu; e1156875@u.nus.edu; qiansoc@nus.edu.sg; qingyunzou@u.nus.edu; bhooi@comp.nus.edu.sg; dcsheb@nus.edu.sg,,Poster 2,WS160
AAAI26_W10_8,CellForge: Agentic Design of Virtual Cell Models,"Virtual cell modeling aims to predict cellular responses to diverse perturbations but faces challenges from biological complexity, multimodal data heterogeneity, and the need for interdisciplinary expertise. We introduce CellForge, a multi-agent framework that autonomously designs and synthesizes neural network architectures tailored to specific single-cell datasets and perturbation tasks. Given raw multi-omics data and task descriptions, CellForge discovers candidate architectures through collaborative reasoning among specialized agents, then generates executable implementations. Our core contribution is the framework itself: showing that multi-agent collaboration mechanisms---rather than manual human design or single-LLM prompting---can autonomously produce executable, high-quality computational methods. This approach goes beyond conventional hyperparameter tuning by enabling entirely new architectural components—such as trajectory-aware encoders and perturbation diffusion modules—to emerge from agentic deliberation.  We evaluate CellForge on six datasets spanning gene knockouts, drug treatments, and cytokine stimulations across multiple modalities (scRNA-seq, scATAC-seq, CITE-seq). The results demonstrate that the models generated by CellForge are highly competitive with established baselines, while revealing systematic patterns of architectural innovation.  CellForge highlights the scientific value of multi-agent frameworks: collaboration among specialized agents enables genuine methodological innovation and executable solutions that single agents or human experts cannot achieve. This represents a paradigm shift toward autonomous scientific method development in computational biology.",Xiangru Tang; Zhuoyun Yu; Jiapeng Chen; Yan Cui; Daniel Shao; Weixu Wang; Fang Wu; Yuchen Zhuang; Wenqi Shi; Zhi Huang; Xihong Lin; Fabian J Theis; Smita Krishnaswamy; Mark Gerstein,xiangru.tang@yale.edu; yukinoshitasherry@gmail.com; jiapeng.chen@yale.edu; yan12@seas.upenn.edu; yanjun.shao@yale.edu; weixu.wang@helmholtz-munich.de; fw2359@columbia.edu; yczhuang@google.com; wenqi.shi@utsouthwestern.edu; zhi.huang@pennmedicine.upenn.edu; xlin@hsph.harvard.edu; fabian.theis@helmholtz-munich.de; smita.krishnaswamy@yale.edu; pi@gersteinlab.org,Oral,Oral 2,
AAAI26_W10_9,ResearchArena-CayleyBench: RL/LLM Benchmark challenges which can advance mathematical research,"This is the third paper of the CayleyPy project applying artificial intelligence methods to problems in group theory.  We announce the first public release of CayleyPy, an open-source Python library for computations with Cayley and Schreier (coset) graphs. Compared with state-of-the-art systems based on classical methods, such as GAP and Sage, CayleyPy handles significantly larger graphs and performs many orders of magnitude times faster.  Using CayleyPy we obtained about 200 new mathematical conjectures on Cayley and Schreier graphs which can be turned into an efficient benchmarks for both RL and LLM models. For many Cayley graphs of symmetric groups $S_n$ we observe quasi-polynomial diameter formulas: a small set of quadratic or linear polynomials indexed by $n \mod s$ and conjecture that it is general phenomenon. We conjecture improved Babai-type bounds of the  diameters by $\frac12 n^2 + 4n$ for undirected case, by $\frac34 n^2 + O(n)$ for directed cases, and by $\frac14 n^2 + O(n)$ for certain Schreier graphs, comparing to prior conjectural bounds of $O(n^2)$. For nilpotent groups we conjecture an improvement of J.S. Ellenberg's results on the diameter of the upper-triangular matrices over $Z/p$, presenting a phenomenon of linear dependence of the diameter with respect to $p$. Moreover, the growth for nilpotent groups is conjectured to follow Gaussian distributions, that is, to exhibit a central limit phenomenon similar to results of P. Diaconis for $S_n$.",Alexander Chervov; Dmytro Fedoriaka; Mark Obozov; Elena V. Konstantinova; Anastasia Sheveleva; Alexander Soibelman; Igor Kiselev; Fedor Levkovich-Maslyuk; Dmitry Volovich; Nick Vilkin-Krom; Alim Bidzhiev; Artem Krasnyi; Mikhail Evseev; Artem Isakov; Michael Litvinov; Elizaveta Geraseva; Liliya Grunwald; Sergei Lytkin; Andrei Smolensky; Sergey Galkin; Eduard Koldunov; Anton Naumov; Ivan Koltsov; Stanislav Diner; Artem Chevychelov; Evelina Kudasheva; Arsenii Sychev; Zakhar Kogan; Altana Natyrova; Ruslan Grimov; Lidia Shishina; Lyudmila Cheldieva; Vladislav Zamkovoy; Dmitrii Kovalenko; Oleg Papulov; Kudashev Sergey; Dmitry Shiltsov; Rustem Turtayev; Olga Nikitina; Dariya Mamayeva; Nikolenko Sergei; Anton Titarenko; Antonina Dolgorukova; Anton Kostin; Alexey Kravchenko; Alexey N. Aparnev; Orianne Debeaupuis; Herve Isambert; Simo Alami Chehboune,al.chervov@gmail.com; fedimser@uw.edu; obozovmark9@gmail.com; e_konsta@math.nsc.ru; anastasiia.sheveleva@math.msu.ru; asoibel@ihes.fr; igor.kiselev@gmail.com; fedor.levkovich-maslyuk@citystgeorges.ac.uk; dmitrii.volovich@mail.huji.ac.il; openreviewacc1@innopolis.ru; bidzhiev@pasteurorg.ru; a.krasniy@criteo.com; mixnota@gmail.com; aoisakov@itmo.ru; litvinovmitch11@gmail.com; liza.geraseva@bk.ru; l.a.grunwald@math.nsc.ru; s.lytkin@kbtu.kz; andrei.smolensky@gmail.com; sergey@puc-rio.br; univmen1@gmail.com; anton.naumov@mvision.ai; ivankolt@gmail.com; stanislav.diner@gmail.com; heavy4evy@gmail.com; evelinn_a@mail.ru; sychev.a.e@edu.mirea.ru; zahar1991@gmail.com; natyrovaaltana@gmail.com; grimovr@gmail.com; openreviewacc15@innopolis.ru; liuda.tarusina@gmail.com; zamkovoyvladislav@gmail.com; caymon14@gmail.com; ob.papulov@gmail.com; openreviewacc10@innopolis.ru; da.shiltsov@gmail.com; rustem.turtayev@alumni.nu.edu.kz; openreviewacc4@innopolis.ru; dmamayeva367@gmail.com; jotting.1.tenuous@icloud.com; openreviewacc13@innopolis.ru; an.dolgorukova@gmail.com; anton.kostin@gmail.com; openreviewacc21@innopolis.ru; openreviewacc6@innopolis.ru; orianne.debeaupuis@curie.fr; herve.isambert@curie.fr; mohamed.alami-chehboune@polytechnique.edu,,Poster 1,WS151
AAAI26_W10_10,Self-Adapting Agent for Automating Research Coding Workflows,"Large Language Models (LLMs) now underlie code assistants that write, debug, and modify non-trivial software with increasing autonomy. However, their reliability degrades sharply on tasks that diverge from standard software engineering benchmarks. This brittleness under distribution shifts is particularly clear in research workflows, where code is less standardized and environments are harder to configure. We focus on research code reproduction as a concrete, measurable instance of this problem and introduce Self-Adapting Research Engineer (SARE), a framework that adapts LLM-based agents at test time through prompt optimization. SARE exposes three editable fields to the agent: a system prompt, a task prompt template, and a persistent cheatsheet of strategies that the agent can consult during execution. These fields are updated by a reflection mechanism that periodically reviews execution logs and metrics from prior runs and proposes targeted edits, guided by a simple global training context that aggregates experience from multiple repositories and unseen tasks. This allows SARE to capture recurring patterns in how research code is structured and run, rather than overfitting to any single repository. We evaluate SARE on SUPER-Bench and ResearchCodeBench which evaluate LLMs for reproducing results on existing research code repositories. We find that test-time adaptation via this reflective prompt optimization framework improves performance by up to 23.6\% on SUPER and 3.5\% on ResearchCodeBench over strong baselines, while remaining cost-effective and surpassing prior state of the art on both benchmarks.",Balaji Dinesh Gangireddi; Aniketh Garikaparthi; Manasi Patwardhan; Arman Cohan,dinesh.gangireddi@tcs.com; aniketh.g@tcs.com; manasi.patwardhan@tcs.com; arman.cohan@yale.edu,,Poster 2,WS155
AAAI26_W10_11,A framework for statistical evaluation of global causal reliability for scientific claim verification,"To facilitate grounded autonomous scientific discovery via AI, it is crucial to ensure that models behave consistently with the evidence pertaining to academic claims. Many works have assessed model reliability over a static set of generated counterfactual perturbations to the input space, then observ- ing if model performance matches expected behavior. How- ever, there was a recent finding showcasing the overall lack of sufficient diversity (Joshi and He 2022) in input modifica- tions leading to poor generalization across some relevant fea- tures and overdependence on non-causal ones. The key con- clusion was to ensure that perturbations are diverse enough to boost generalization and representativeness of the popula- tion. Given the importance for maintaining a well-balanced, representative set of perturbations consistently reflecting the inherent variability of the global perturbation space and work- ing in a high-stakes scientific literature domain, we choose to formally quantify model causal reliability over the distri- bution of the full population of possible perturbations using a bounded statistical sample estimator for strong scientific claim verification. Existing statistical robustness frameworks mainly require certain assumptions to withhold such as local smoothness, continuous metric spaces, or Lipschitz continu- ity which is not ideal for testing a diverse array of counter- factual perturbations in large language models. While recent work (Levy, Ashrov, and Katz 2025) worked on quantify- ing a reliability estimate by adapting (Levy and Katz 2022) to large language models, their invariant robustness metric is focused on local, small, semantic-preserving modifications along with expensive marginal distribution generation for ev- ery input while assuming continuous neighborhood structure. Their method would not apply well towards our goals. Unlike existing works that measure pure local robustness, our work is the first to establish a new evaluation paradigm for measur- ing global causal reliability of models conducting scientific discovery across the full population of diverse counterfactual perturbations.",Deepansha Singh,ds2456@cornell.edu,,,
AAAI26_W10_12,Position on LLM-Assisted Peer Review: Addressing Reviewer Gap through Mentoring and Feedback,"The rapid expansion of AI research has intensified the Reviewer Gap, threatening the sustainability of the peer-review system and fueling a cycle of low-quality evaluations. This position paper critiques existing LLM approaches that attempt to automatically generate reviews and argues for a paradigm shift that positions LLMs as tools for assisting and educating human reviewers. We define the core principles of high-quality peer review and propose two complementary systems built on these foundations: (i) an LLM-assisted mentoring system that develops reviewers’ long-term competencies and (ii) an LLM-assisted feedback system that helps reviewers refine the quality of their actual reviews. This human-centered approach aims to strengthen reviewer expertise, address the structural roots of the Reviewer Gap, and support the creation of a more sustainable scholarly ecosystem.",JungMin Yun; Junehyoung Kwon; MiHyeon Kim; YoungBin Kim,cocoro357@cau.ac.kr; dirchdmltnv@cau.ac.kr; mh10967@gmail.com; ybkim85@cau.ac.kr,,Poster 2,WS151
AAAI26_W10_13,Human-in-Command Governance for Multi-Agent Scientific Workflows,"While AI can accelerate text-intensive research, unsupervised autonomous agents risk compounding errors and cannot be held accountable for the integrity of their findings. Accordingly, we propose a human-supervised, multi-agent model for Human-AI collaboration in science that reconciles the power of Large Language Models (LLMs) with the core principles of transparency and accountability. In our vision, a human researcher supervises a team of specialized AI agents that execute discrete tasks within a rigorously defined scientific methodology. Each step in the process is governed by human expert validation, in order to prevent error propagation and maintain conceptual integrity.  We instantiate this model in an end-to-end framework for systematic taxonomy generation by operationalizing the Nickerson et al. (2013) methodology. We provide a proof-of-concept (PoC) implementation and a qualitative case study that validates our framework by re-deriving a recently published taxonomy. Our analysis documents the framework's fidelity to the original method and illustrates how human-gated oversight is crucial for ensuring the scientific rigor of the final taxonomy. To support reproducibility, we release our prompts, agent roles, and workflow design as a generalizable blueprint for structuring human-AI teams. This work argues that method-bounded human-in-command (HIC) supervision offers a principled and practical path to developing reliable and accountable AI research assistants.",Samed Bayer; Ingo Weber,samed.bayer@tum.de; ingo.weber@tum.de,,Poster 1,WS160
AAAI26_W10_14,Neural Cellular Automata and the PD-NCA AutoLab: A Differentiable Artificial Life Testbed for Autonomous Research Agents,"We present a differentiable artificial-life testbed that enables autonomous research agents to generate hypotheses, run ab initio experiments, and evaluate emergent phenomena. Current systems, while proficient at automating specific tasks, are often constrained by the lack of generative environments suitable for novel, ab initio experimentation. We introduce the PD-NCA AutoLab, a conceptual architecture for an autonomous research environment built upon a Programmable, Differentiable Neural Cellular Automaton (PD-NCA). This “differentiable petri dish” allows agents to investigate and modify the environment’s underlying physics, which are encoded as neural networks.  Contributions: (1) PD-NCA AutoLab, a programmable, differentiable cellular-automata environment that exposes its “physics” for agent-driven design; (2) a multi-agent research workflow (Hypothesis → Execute → Analyze → Reflect) that operationalizes autonomous experimentation; and (3) preliminary feasibility studies demonstrating stable BPTT gradients and discovery sparsity that motivates intelligent search.",David Scott Lewis; Enrique Zueco,david.lewis@aiexecutiveconsulting.com; enrique.zueco@aiexecutiveconsulting.com,,,
AAAI26_W10_15,MedRGB: Practical Framework for Benchmarking Medical Retrieval-Augmented Generation Systems,"Retrieval-augmented generation (RAG) has emerged as a promising approach to enhance the performance of large language models (LLMs) in knowledge-intensive tasks such as those from medical domain. However, the sensitive nature of the medical domain necessitates a completely accurate and trustworthy system. While existing RAG benchmarks primarily focus on the standard retrieve-answer setting, they overlook many practical scenarios that measure crucial aspects of a reliable medical system. This paper addresses this gap by introducing a practical benchmarking framework called Medical Retrieval-Augmented Generation Benchmark (MedRGB) that provides various supplementary elements to handle these specific scenarios. In particular, MedRGB measures the capacity of medical question-answering (QA) systems in a RAG setting in terms of not only accuracy, but also sufficiency, integration, and robustness. We utilize MedRGB to conduct extensive evaluations of both state-of-the-art commercial LLMs and open-source models across multiple retrieval conditions. Our experimental results on four different medical QA datasets reveals current models' limited ability to handle noise and misinformation in the retrieved documents. We further analyze the LLMs' reasoning processes to provides valuable insights and future directions for developing RAG systems in this critical medical domain",Nghia Trung Ngo; Chien Van Nguyen; Franck Dernoncourt; Thien Huu Nguyen,nghian@uoregon.edu; chienn@uoregon.edu; franck.dernoncourt@gmail.com; thienn@uoregon.edu,,,
AAAI26_W10_16,The Triangle of Similarity: A Multi-Faceted Framework for Comparing Neural Network Representations,"Comparing neural network representations is essential for understanding and validating models in scientific applications. Existing methods, however, often provide a limited view. We propose the Triangle of Similarity, a framework that combines three complementary perspectives: static representational similarity (CKA/Procrustes), functional similarity (Linear Mode Connectivity or Predictive Similarity), and sparsity similarity (robustness under pruning). Analyzing a range of CNNs, Vision Transformers, and Vision-Language Models, our initial findings suggest that: (1) architectural family is a primary determinant of representational similarity, forming distinct clusters; (2) CKA self-similarity and task accuracy are strongly correlated during pruning, though accuracy often degrades more sharply; and (3) for some model pairs, pruning appears to regularize representations, exposing a papershared computational core. This framework offers a more holistic approach for assessing whether models have converged on similar internal mechanisms, providing a useful tool for model selection and analysis in scientific research.",Olha Sirikova; Alvin Chan,olhasiri@gmail.com; guoweialvin.chan@ntu.edu.sg,,,
AAAI26_W10_17,QuantumChem-200K: A Large-Scale Open Organic Molecular Dataset for Quantum-Chemistry Property Screening and Benchmarking,"The discovery of next-generation photoinitiators for two-photon polymerization (TPP) is hindered by the absence of large, open datasets containing the quantum-chemical and photophysical properties required to model photodissociation and excited-state behavior. Existing molecular datasets typically provide only basic physicochemical descriptors and therefore cannot support data-driven screening or AI-assisted design of photoinitiators. To address this gap, we introduce QuantumChem-200K, a large-scale dataset of over 200,000 organic molecules annotated with 11 mechanistically relevant properties, including two-photon absorption (TPA) cross sections, TPA spectral ranges, singlet–triplet intersystem crossing (ISC) energies, toxicity and synthetic accessibility scores, hydrophilicity, solubility, boiling point, molecular weight, and aromaticity. These values are computed using a hybrid workflow that integrates density function theory (DFT), semi- empirical excited-state methods, atomistic quantum solvers, and neural-network predictors. Using QuantumChem-200K, we fine-tune the open-source Qwen-2.5-32B large language model to create a chemistry AI assistant capable of forward property prediction from SMILES. Benchmarking on 3000 unseen molecules from VQM24 and ZINC20 demonstrates that domain-specific fine-tuning significantly improves accuracy over GPT-4o, Llama-3.1-70B, and the base Qwen2.5-32B model, particularly for TPA and ISC predictions central to photoinitiator design. QuantumChem-200K and the corresponding AI assistant together provide the first scalable plat- form for high-throughput, LLM-driven photoinitiator screening and accelerated discovery of photo-sensitive materials.",Yinqi Zeng; Renjie Li,yinqiz2@illinois.edu; renjie2@illinois.edu,,Poster 2,WS152
AAAI26_W10_18,Does Less Hallucination Mean Less Creativity? An Empirical Investigation in LLMs,"Large Language Models (LLMs) exhibit remarkable capabilities in natural language understanding and reasoning, but suffer from hallucination: the generation of factually incorrect content. While numerous methods have been developed to reduce hallucinations, their impact on creative generations remains unexplored. This gap is particularly critical for AI-assisted scientific discovery, which requires both factual accuracy and creative hypothesis generation. We investigate how three hallucination-reduction techniques: Chain of Verification (CoVe), Decoding by Contrasting Layers (DoLa), and Retrieval-Augmented Generation (RAG), affect creativity in LLMs. Evaluating multiple model families (LLaMA, Qwen, Mistral) at varying scales (1B - 70B parameters) on two creativity benchmarks (NeoCoder and CS4), we find that these methods have opposing effects on divergent creativity. CoVe enhances divergent thinking, DoLa suppresses it, and RAG shows minimal impact. Our findings provide guidance for selecting appropriate hallucination-reduction methods in scientific applications, where the balance between factual accuracy and creative exploration is crucial.",Nadya Yuki Wangsajaya; Banerjee Mohor; Syed Ali Redha Alsagoff; Tan Min Sen; Zachary Choy Kit Chun; Alvin Chan,nady0006@e.ntu.edu.sg; mohor001@e.ntu.edu.sg; syedalir001@e.ntu.edu.sg; minsen2310@gmail.com; 26yzach623z@student.ri.edu.sg; guoweialvin.chan@ntu.edu.sg,Oral,Oral 3,
AAAI26_W10_19,Cross-Modal Cell Cycle Phase Classification via Dual-Encoder Transfer Learning with Foundation Model Embeddings,"Cell cycle phase identification is essential for understanding cellular proliferation, disease progression, and treatment response in single-cell genomics. While single-cell RNA sequencing (scRNA-seq) enables diverse cell cycle classification strategies, single-cell ATAC sequencing (scATAC-seq) offers complementary epigenetic insights but lacks well-annotated datasets for supervised learning. We present a cross-modal transfer learning framework that leverages paired multiome data to transfer cell cycle knowledge from gene expression to chromatin accessibility domains. Through systematic evaluation of ten single-cell foundation models spanning 10M to 400M parameters (Geneformer, scGPT, scFoundation, UCE, TEDDY), we identify Geneformer-316M as optimal for cross-modal transfer with domain-adversarial neural networks. Our framework integrates class-conditional domain adaptation with diversity-aware training using KL, Jensen–Shannon, CORAL, and MMD divergence to prevent target-domain collapse and stabilize cross-modal learning. Across four divergence measures and eight peak encoder architectures, our models achieve strong cross-dataset performance on SUP-B15 scRNA-seq, and with MMD we obtain robust cross-modal transfer to scATAC-seq, reaching competitive accuracy on REH and generalizing to SUP-B15 despite extreme peak sparsity.",Halima Akhter; Donald Adjeroh; Gangqing Hu,ha00014@mix.wvu.edu; donald.adjeroh@mail.wvu.edu; michael.hu@hsc.wvu.edu,,,
AAAI26_W10_20,Autonomous LLM Agents for Causal Estimation and Model Refinement,"Causal inference is central to scientific and policy decision-making, yet a major challenge in observational studies is separating true treatment effects from confounding, especially when confounders are high-dimensional or text- and image-based. Despite methodological progress, researchers still struggle with two persistent questions: *which confounders to adjust for* and *which estimators to use*. Here, we test whether large language models (LLMs) can autonomously design and refine causal estimators. We introduce a causal-agent framework with two modes: a zero-shot LLM that proposes estimators from structured prompts, and a self-evolving agent that iteratively improves them using evaluation feedback. Across two benchmarks for heterogeneous treatment effect estimation, the self-evolving agent matched or exceeded human-expert performance, achieving lower RMSE and near-nominal confidence interval coverage, and substantially outperforming zero-shot LLM solutions. Examination of the agent’s updates showed that it progressively learned appropriate specifications: early iterations added regularization and pre-outcome adjustment; mid-stage refinements introduced interactions, weighting, and clustered inference; and later steps converged toward augmented methods that balanced accuracy and calibration. Our results suggest that LLM-based agents can autonomously explore and optimize causal estimators, narrowing the gap between automated and expert-driven analysis. Our next steps are to evaluate generalization to additional causal tasks and to assess whether providing agents with access to advanced causal inference packages can further enhance performance.",Can Wang; Yiqun T. Chen,cwang271@jh.edu; yiqunc@jhu.edu,Oral,Oral 4,
AAAI26_W10_21,Autonomous AI Assistants for Scientific Research: Physics-Aware Foundations and Climate Downscaling Case Study,"Modern scientific AI models excel at pattern prediction but rarely verify physical consistency or refine their hypotheses. This limitation is acute in climate downscaling, where fine-scale fields must respect conservation structure and spatiotemporal dynamics. Physics-Informed Neural Networks (PINNs) embed a partial differential equation (PDE) structure through residual penalties and work well when the governing equations are complete. However, in realistic climate systems with partially known or parameterized operators, enforcing an imperfect PDE can introduce bias and destabilize optimization.  Therefore, we introduce a \emph{physics-aware autonomous AI assistant} that treats predictions as hypotheses to be validated and corrected in a closed loop. Rather than relying on a fixed PDE residual, the assistant uses \emph{physical validators}: soft modular diagnostics whose gradients supply adaptive curvature to the learning dynamics. These validators induce strong monotonicity in directions of physical violation and create a dissipative Lyapunov structure, yielding stable, contractive refinement without requiring a fully specified governing equation.  Experiments based on our approach on a synthetic controlled 1D diffusion-advection benchmark generated using a stable Forward-Time–Central-Space solver show consistent gains over an architecture-matched PINN baseline, lower bias and variance, smoother error heat maps, slower horizon-wise error growth, improved local curvature recovery, and self-regulated validator weights that decay from $\lambda\approx 5$ to near zero. These results demonstrate that validator-driven refinement is a flexible and stable foundation for interpretable, autonomous scientific AI systems.",Jaskeerat Singh; Gagandeep Kaur,jaskeerat@gmail.com; gagandeepk@iiitd.ac.in,,Poster 1,WS158
AAAI26_W10_22,Beyond Protein Language Models: An Agentic LLM Framework for Mechanistic Enzyme Design,"We present Genie-CAT, a tool-augmented large-language-model (LLM) system designed to accelerate scientific hypothesis generation in protein design. Using metalloproteins (e.g., ferredoxins) as a case study, Genie-CAT integrates four capabilities---literature-grounded reasoning through retrieval-augmented generation (RAG), structural parsing of Protein Data Bank files, electrostatic potential calculations, and machine-learning prediction of redox properties---into a unified agentic workflow. By coupling natural-language reasoning with data-driven and physics-based computation, the system generates mechanistically interpretable, testable hypotheses linking sequence, structure, and function. In proof-of-concept demonstrations, Genie-CAT autonomously identifies residue-level modifications near [Fe--S] clusters that affect redox tuning, reproducing expert-derived hypotheses in a fraction of the time. The framework highlights how AI agents combining language models with domain-specific tools can bridge symbolic reasoning and numerical simulation, transforming LLMs from conversational assistants into partners for computational discovery.",Bruno Jacob; Khushbu Agarwal; Marcel Baer; Peter Rice; Simone Raugei,bruno.jacob@pnnl.gov; khushbu.agarawl@pnnl.gov; marcel.baer@pnnl.gov; peter.rice@pnnl.gov; simone.raugei@pnnl.gov,,,
AAAI26_W10_23,"A Unified Theoretical Framework for AI-Empowered Research: Unifying Ecosystems, Processes, Collaboration and Thinking Paradigms","Despite the increasing application of Artificial Intelligence (AI) in scientific research, the field lacks a unified theoretical framework to systematically guide and understand this transformation, leading to a disconnect between practice and theory. To bridge this gap, this paper proposes a unified, multi-level framework for AI-empowered research. The framework first deconstructs the AI-driven research ecosystem into a three-stage process (pre-paper, corpus-centric, post-paper) and proposes the three evolving human-AI collaboration model (tool, assistant, and participant) that underpin these stages. Besides, this work introduces the Al-empowered  research thinking paradigms, which elaborates how AI influences and empowers the core cognitive capacities of researchers (3C: critical, creative, and computational thinking). The framework's validity is substantiated through a two-pronged approach: a theoretical validation of the full lifecycle via the AlphaFold case, and an empirical validation of the framework's core human-AI collaboration models through our Paper2Presentation case study. This work offers a comprehensive roadmap for understanding and building the next generation of human-AI collaborative research platforms.",Feng Xiong; Xinguo Yu; Hon Wai Leong; Anran Ma,mountain@mails.ccnu.edu.cn; xgyu@ccnu.edu.cn; leonghw@comp.nus.edu.sg; lingsu@ccnu.edu.cn,,Poster 1,WS155
AAAI26_W10_24,Evaluation of LLM Agents for Research Mentorship,"Many students and early-career researchers lack access to expert research mentorship. We study whether an AI mentor can help undergraduates progress from idea to paper. We built MENTOR, a tool-augmented, stage-aware assistant that integrates literature search, curated guidelines, methodology checks, and memory. We evaluate MENTOR across six writing stages using two LLM-as-a-judge protocols: (i) expert-persona pairwise preferences and (ii) student-persona rubric scores, plus short multi-turn tutoring tasks and evidence/compliance checks. Across 90 single-turn comparisons, LLM judges preferred MENTOR to Claude Sonnet 4.5 in 74.4% (95% CI [64.3, 82.5]) and to GPT-5 in 58.3% (95% CI [47.7, 68.3]). MENTOR's student-persona scores (clarity, actionability, constraint-fit) are consistently higher than both baselines across stages. In multi-turn sessions (five scenarios/agent), MENTOR achieved slightly higher final quality than GPT-5 under our rubric.",Abhinav Rajeev Kumar; Dhruv Trehan; Paras Chopra,abku2504@gmail.com; dhruv.trehan@alumni.ashoka.edu.in; paras@lossfunk.com,,Poster 2,WS154
AAAI26_W10_25,LLM4Laser: Large Language Models Automate the Design of Lasers,"With the rapid evolution of global autonomous driving technology, the demand for its core sensing hardware, Light Detection and Ranging (LiDAR), is escalating. As the light source part of the LiDAR system, lasers, particularly the cutting-edge Photonic Crystal Surface Emitting Lasers (PCSEL), have correspondingly attracted extensive research attention. The conventional manual design and optimization of PCSEL typically require expertise in semiconductor physics and months of dedicated effort to achieve satisfactory results. While AI-driven approaches can expedite this pro- cess, laser designers still need to invest time in learning the AI algorithms involved. Meanwhile Large Language Models (LLMs), leveraging their powerful reasoning abilities, can effectively comprehend natural language and provide con- structive feedback in multi-turn dialogues. They have already demonstrated potential to assist humans in scientific fields such as robotics design and chemical discovery. A question naturally arises is: Can LLMs transform the lasers design process? This paper proposes a novel human-AI co-design paradigm to show that LLMs can guide the laser design and optimization process both conceptually and technically. Specifically, by simply having conversations, GPT assisted us with writing both Finite Difference Time Domain (FDTD) simulation code and deep reinforcement learning code to acquire the optimized PCSEL solution, spanning from the proposition of ideas to the realization of algorithms. Given that GPT will perform better when given detailed and specific prompts, we break down the PCSEL design problem into a series of sub-problems and converse with GPT by posing open-ended heuristic questions rather than definitive commands. We achieved a significant milestone towards self-driving laboratories, that is, a fully automated AI-driven pipeline, for laser design and production.",Renjie Li; Ceyao Zhang; Sixuan Mao; Xiyuan Zhou; Feng Yin; Sergios Theodoridis; Zhaoyu Zhang,renjie2@illinois.edu; ceyaozhang@gmail.com; 121090414@link.cuhk.edu.cn; 120090649@link.cuhk.edu.cn; yinfeng@cuhk.edu.cn; stheodor@di.uoa.gr; zhangzy@cuhk.edu.cn,,Poster 2,WS153
AAAI26_W10_26,When CNNs Outperform Transformers and Mambas: Revisiting Deep Architectures for Dental Caries Segmentation,"Accurate identification and segmentation of dental caries in panoramic radiographs are critical for early diagnosis and effective treatment planning. Automated segmentation remains challenging due to low lesion contrast, morphological variability, and limited annotated data. In this study, we present the first comprehensive benchmarking of convolutional neural networks, vision transformers and state-space mamba architectures for automated dental caries segmentation on panoramic radiographs through a DC1000 dataset. Twelve state-of-the-art architectures, including VMUnet, MambaUNet, VMUNetv2, RMAMamba-S, TransNetR, PVTFormer, DoubleU-Net, and ResUNet++, were trained under identical configurations. Results reveal that, contrary to the growing trend toward complex attention based architectures, the CNN-based DoubleU-Net achieved the highest dice coefficient of 0.7345, mIoU of 0.5978, and precision of 0.8145, outperforming all transformer and Mamba variants. In the study, the top 3 results across all performance metrics were achieved by CNN-based architectures.  Here, Mamba and transformer-based methods, despite their theoretical advantage in global context modeling, underperformed due to limited data and weaker spatial priors. These findings underscore the importance of architecture-task alignment in domain-specific medical image segmentation more than model complexity. We will make all the code of the models open access upon acceptance.",Aashish Ghimire; Jun Zeng; Roshan Paudel; Nikhil Kumar Tomar; Deepak Ranjan Nayak; Harshith Reddy Nalla; Vivek Jha; Glenda Reynolds; Debesh Jha,aashish.ghimire@coyotes.usd.edu; zeng.cqupt@gmail.com; roshan.paudel@coyotes.usd.edu; nikhilroxtomar@gmail.com; drnayak.cse@mnit.ac.in; harshithreddy.nalla@coyotes.usd.edu; vivekbpkihs@gmail.com; glenda.reynolds@usd.edu; debeshjha1@gmail.com,,,
AAAI26_W10_27,"Agentic Repair of Gurobi Optimization Models via Tool Use: A Fractional-Factorial Study of Knowledge, Diagnostics, and Execution","We present a modular agentic framework for automatic inspection and repair of Gurobi-based optimization code.  The language model operates as an orchestrator over predefined tools and executes iterative reasoning through a closed loop of diagnosis, tool invocation, and verification.  To measure the contribution of different functional tool capabilities, we apply a fractional-factorial experimental design using a newly constructed dataset of 26 Gurobi optimization tasks, evaluated across three random seeds, two model variants, and two reasoning budgets (12 vs.\ 24 steps). Our analysis shows that reasoning depth is the dominant factor governing repair accuracy: under shallow reasoning, knowledge-oriented retrieval and diagnostic tools yield substantial improvements in solver and validation performance, while under deeper reasoning their marginal benefit diminishes and execution feedback contributes little additional value.",Ke Zhang; Maziar Raissi,kzhan153@ucr.edu; maziar.raissi@gmail.com,,,
AAAI26_W10_28,From Verification Burden to Trusted Collaboration: Design Goals for LLM-Assisted Literature Reviews,"Large Language Models (LLMs) are increasingly embedded in academic writing practices. Although numerous studies have explored how researchers employ these tools for scientific writing, their concrete implementation, limitations, and design challenges within the literature review process remain underexplored. In this paper, we report a user study with researchers across multiple disciplines to characterize current practices, benefits, and \textit{pain points} in using LLMs to investigate related work. We identified three recurring gaps: (i) lack of trust in outputs, (ii) persistent verification burden, and (iii) requiring multiple tools. This motivates our proposal of six design goals and a high-level framework that operationalizes them through improved related papers visualization, verification at every step, and human-feedback alignment with generation-guided explanations. Overall, by grounding our work in the practical, day-to-day needs of researchers, we designed a framework that addresses these limitations and models real-world LLM-assisted writing, advancing trust through verifiable actions and fostering practical collaboration between researchers and AI systems.",Brenda Nogueira; Werner Geyer; Andrew Anderson; Toby Jia-Jun Li; Dongwhi Kim; Nuno Moniz; Nitesh V Chawla,bcruznog@nd.edu; werner.geyer@us.ibm.com; andrew.anderson2@ibm.com; tobyli@cs.cmu.edu; dkim37@nd.edu; nmoniz2@nd.edu; nchawla@nd.edu,,Poster 1,WS157
AAAI26_W10_29,RiverScope: High-Resolution River Masking Dataset,"Surface water dynamics play a critical role in Earth's climate system, influencing ecosystems, agriculture, disaster resilience, and sustainable development. Yet monitoring rivers and surface water at fine spatial and temporal scales remains challenging---especially for narrow or sediment-rich rivers that are poorly captured by low-resolution satellite data. To address this, we introduce RiverScope, a high-resolution dataset developed through collaboration between computer science and hydrology experts. RiverScope comprises 1,145 high-resolution images (covering 2,577 square kilometers) with expert-labeled river and surface water masks, requiring over 100 hours of manual annotation. Each image is co-registered with Sentinel-2, SWOT, and the SWOT River Database (SWORD), enabling the evaluation of cost-accuracy trade-offs across sensors---a key consideration for operational water monitoring. We also establish the first global, high-resolution benchmark for river width estimation, achieving a median error of 7.2 meters---significantly outperforming existing satellite-derived methods. We extensively evaluate deep networks across multiple architectures (e.g., CNNs and transformers), pretraining strategies (e.g., supervised and self-supervised), and training datasets (e.g., ImageNet and satellite imagery). Our best-performing models combine the benefits of transfer learning with the use of all the multispectral PlanetScope channels via learned adaptors. RiverScope provides a valuable resource for fine-scale and multi-sensor hydrological modeling, supporting climate adaptation and sustainable water management.",Rangel Daroya; Taylor Rowley; Jonathan Acero Flores; Elisa Friedmann; Fiona B Bennitt; Heejin An; Travis Thomas Simmons; Marissa Hughes; Camryn L Kluetmeier; Solomon Kica; J. Daniel Vélez; Sarah E. Esenther; Thomas E. Howard; Yanqi Ye; Audrey J. Turcotte; Colin Gleason; Subhransu Maji,rdaroya@umass.edu; throwley@umass.edu; jflores@umass.edu; efriedmann@umass.edu; fbennitt@umass.edu; heejinan@umass.edu; tsimmons@umass.edu; mjdudek@email.unc.edu; ckluet@unc.edu; kica@unc.edu; davelez@unc.edu; sarah_esenther@brown.edu; thomas_howard@brown.edu; yanqi_ye@brown.edu; audrey.turcotte@colorado.edu; cjgleason@umass.edu; smaji@cs.umass.edu,Oral,Oral 5,
AAAI26_W10_30,Genomic heterogeneity inflates the performance of variant pathogenicity predictions,"Recent studies have reported unprecedented accuracy predicting pathogenic variants across the genome, including in noncoding regions, using large AI models trained on vast genomic data. We present a comprehensive evaluation of these frontier models, showing that performance is inflated by differences in the prevalence of pathogenic variants across genomic contexts. We identify the best-performing models for each variant type and establish a benchmark to guide future progress.",Baiyu Lu; Xueshen Liu; Po-Yu Lin; Nadav Brandes,baiyul@uci.edu; xl5279@nyu.edu; po-yu.lin@nyulangone.org; nadav.brandes@nyulangone.org,,Poster 1,WS154
AAAI26_W10_31,ViInfographicVQA: A Benchmark for Single and Multi-image Visual Question Answering on Vietnamese Infographics,"Infographic Visual Question Answering (InfographicVQA) evaluates a model’s ability to read and reason over data-rich, layout-heavy visuals that combine text, charts, icons, and design elements. Compared with scene-text or natural-image VQA, infographics require stronger integration of OCR, layout understanding, and numerical and semantic reasoning. We introduce ViInfographicVQA, the first benchmark for Vietnamese InfographicVQA, comprising over 6747 real-world infographics and 20409 human-verified question-answer pairs across economics, healthcare, education, and more. The benchmark includes two evaluation settings. The Single-image task follows the traditional setup in which each question is answered using a single infographic. The Multi-image task requires synthesizing evidence across multiple semantically related infographics and is, to our knowledge, the first Vietnamese evaluation of cross-image reasoning in VQA. We evaluate a range of recent vision-language models on this benchmark, revealing substantial performance disparities, with the most significant errors occurring on Multi-image questions that involve cross-image integration and non-span reasoning. ViInfographicVQA contributes benchmark results for Vietnamese InfographicVQA and sheds light on the limitations of current multimodal models in low- resource contexts, encouraging future exploration of layout-aware and cross-image reasoning methods.",Tue-Thu Van-Dinh; Duy Hoang Tran; Truong-Binh Duong; Mai-Hanh Pham; Nam Nguyen Binh Le; Quoc-Thai Nguyen,thuhoctap1713@gmail.com; duytranus5300@gmail.com; duongtruongbinh2003@gmail.com; maihanhpham.coco@gmail.com; lnbinhnamhcmus@gmail.com; quocthai@neu.edu.vn,,Poster 1,WS159
AAAI26_W10_32,Uncertainty-Aware Autonomous Vehicles: Predicting the Road Ahead,"Autonomous Vehicle (AV) perception systems have advanced rapidly in recent years, providing vehicles with the ability to accurately interpret their environment. Perception systems remain susceptible to errors caused by overly-confident predictions in the case of rare events or out-of-sample data. This study equips an autonomous vehicle with the ability to 'know when it is uncertain', using an uncertainty-aware image classifier as part of the AV software stack. Specifically, the study exploits the ability of Random-Set Neural Networks (RS-NNs) to explicitly quantify prediction uncertainty. Unlike traditional CNNs or Bayesian methods, RS-NNs predict belief functions over sets of classes, allowing the system to identify and signal uncertainty clearly in novel or ambiguous scenarios. The system is tested in a real-world autonomous racing vehicle software stack, with the RS-NN classifying the layout of the road ahead and providing the associated uncertainty of the prediction. Performance of the RS-NN under a range of road conditions is compared against traditional CNN and Bayesian neural networks, with the RS-NN achieving significantly higher accuracy and superior uncertainty calibration. This integration of RS-NNs into Robot Operating System (ROS)-based vehicle control pipeline demonstrates that predictive uncertainty can dynamically modulate vehicle speed, maintaining high-speed performance under confident predictions while proactively improving safety through speed reductions in uncertain scenarios. These results demonstrate the potential of uncertainty-aware neural networks - in particular RS-NNs - as a practical solution for safer and more robust autonomous driving.",Shireen Kudukkil Manchingal; Armand Amaritei; Mihir Gohad; Maryam Sultana; Julian F. P. Kooij; Fabio Cuzzolin; Andrew Bradley,smanchingal@brookes.ac.uk; 19205454@brookes.ac.uk; p0097038@brookes.ac.uk; msultana@brookes.ac.uk; j.f.p.kooij@tudelft.nl; fabio.cuzzolin@brookes.ac.uk; abradley@brookes.ac.uk,,,
AAAI26_W10_33,PathSymetic: Neuro-Symbolic Causal Hypothesis Generation for Mechanistic Pathway Discovery in Genomic Systems,"Identifying causal relationships between gene-level signals and biological pathways remains a core challenge in functional genomics, particularly under high-dimensional and noisy transcriptomic data. PathSymetic is a neuro-symbolic framework that integrates large language models (LLMs), ontology-grounded knowledge graphs, and causal structure learning to infer interpretable pathway-level hypotheses. It combines symbolic reasoning and neural representations in three stages: (1) Ontology-guided symbolic grounding, where pathway and reaction metadata are structured into logical graphs; (2) Causal representation alignment, where interventional transcriptomic data (e.g., CRISPR, small-molecule perturbations) are used to learn causal attributions via counterfactual probing; and (3) Concept-level hypothe-sis generation, where symbolic rules are merged with LLM-derived latent embeddings to yield ranked mechanistic pathway hypotheses. Fine-tuned on benchmark datasets from cancer and metabolic diseases, PathSymetic achieved an AUPR of 0.81, F1-score of 0.77, and Precision@10 of 0.94, outperforming attention-based GNNs (AUPR 0.68) and pathway enrichment baselines (F1 0.52). It further achieves Hit@10 of 98.4 percent and Hit@20 of 99.6 percent, highlighting its ability to rank experimentally validated pathways among top candidates. It prioritizes experimentally validated pathways among top predictions and uncovers biologically plausible novel hypotheses, supported by co-citation analysis and mechanistic literature.",Harshini Suresha,pes1202203941@pesu.pes.edu,,,
AAAI26_W10_34,Data Verification is the Future of Quantum Computing Copilots,"Quantum program generation demands mathematical precision incompatible with the statistical reasoning carried out by large language models (LLMs). Hallucinations are mathematically inevitable, instead of engineering problems solvable by scale. We argue that architectures prioritizing verification are necessary for quantum copilots and AI automation in domains governed by constraints. Our position rests on three points: verified training data enables models to internalize precise constraints as learned structure rather than statistical approximation; verification must constrain generation rather than filter outputs, as valid designs occupy exponentially shrinking subspaces; and domains where physical laws impose correctness criteria require verification embedded as architectural primitives. Early experiments show LLMs with verification knowledge achieve more than 79% accuracy on circuit optimization. Our positions are formulated as quantum computing and AI4Research community imperatives, calling for elevating verification from afterthought to architectural foundation in AI4Research.",Junhao Song; Ziqian Bi; Chia Xin Liang; William Knottenbelt; Yudong Cao,junhao.song23@imperial.ac.uk; bi32@purdue.edu; marcus@navigent.ai; w.knottenbelt@imperial.ac.uk; cao.yudong@bcg.com,,,
AAAI26_W10_35,"CertiHealth: Towards Certified, Uncertainty-Aware and Explainable AI for Medical Decision Making","Artificial intelligence has shown promise in automating clinical diagnosis and decision support, yet most medical AI systems remain unreliable, opaque, and unverified [1, 4, 5]. Existing approaches typically address either model interpretability [6, 7], uncertainty quantification [3], or robustness certification [1, 2] in isolation, leaving critical gaps in safety and trust [11, 12]. This paper introduces CertiHealth, a unified framework for developing certified, uncertainty-aware, and explainable AI models for medical decision-making. CertiHealth constrains neural architectures through Lipschitz continuity [1], enabling formal robustness guarantees against bounded input perturbations. It integrates probabilistic uncertainty estimation [3] to assess predictive confidence and employs interpretable attribution mechanisms [6, 7] to provide transparent, clinically meaningful explanations. Evaluated on diagnostic tasks using the MIMIC-IV clinical dataset, CertiHealth demonstrates improved calibration, verifiable robustness, and alignment between model explanations and known medical risk factors. By combining mathematical certification, quantified uncertainty, and human-centered interpretability [8, 9, 13], CertiHealth advances the development of verifiably trustworthy medical AI suitable for safety-critical healthcare environments [10, 12, 14].",Bisiriyu Maqsudhat Kofoworola,bisiriyumk.22@student.funaab.edu.ng,,,
AAAI26_W10_36,Autoformalization of Origins of Life: From Scientific Prose to Typed Horn Programs,"We extend autoformalization into natural sciences prose by extracting axiom-like statements from scientific text, compiling them into a typed Horn program, and exhaustively enumerating propositions implied by the model. We use Expert-in-the-Loop approach to map sentences typed facts and Horn rules and ultimately to produce domain-specific Horn program schema. We exhaustively enumerate non-empty subsets of the axioms and run a forward-chaining engine to produce the set of reachable hypotheses along with their minimal axiomatic supports. As a case study, we apply this approach to Origins of Life, specifically the problem of the origin of translation. Here, we formalize an earlier proposed exaptation hypothesis, factorize an implicit signaling function, and derive a novel ""signaling-first"" pathway to translation that was not made explicit in the source prose. In this framing, the autoformalization phase includes construction of the typed Horn program schema, whereas the execution of program instances to closure is general. The result is a general, reproducible path from scientific narratives to verifiable hypothesis generation with explicit accounting for dependencies and alternatives.",Dmitry Zubarev,dmitry.zubarev@ibm.com,,Poster 2,WS158
AAAI26_W10_37,Overcoming Combinatorial Explosion in Alloy Design via Hierarchical Multi-Agent Systems,"Traditional AI-driven materials discovery pipelines employ a monolithic architecture where a single surrogate model is trained, scalarized, and deployed statically, creating a brittle interface with physical experimentation. We present a hierarchical multi-agent system (MAS) that fundamentally redesigns this architecture through three innovative mechanisms: (1) furnace-to-agent feedback loops enabling continuous online learning, (2) a curiosity-annealing scheduler for adaptive exploration-exploitation balance, and (3) memory injected composition generators that leverage historical success. This architectural approach reduces required physical lab iterations by seven-fold compared to the best-performing static multi-agent baseline (AtomAgent). The system identified and experimentally validated 21 novel Pareto-optimal alloys that outperform canonical benchmarks (Ti-6Al-4V, Inconel-718, Cantor HEA) while maintaining 97% metallurgical feasibility. These gains demonstrate that architectural innovation, rather than model scale or compute budget, drives the next frontier of AI-accelerated scientific discovery, offering a template for transforming high-cost experimental domains.",Mahule Roy; Subhas Roy,roymahule26@gmail.com; roysubhas69@gmail.com,,,
AAAI26_W10_38,An MLLM-based AI Assistant for Autonomous Equation Discovery from Visual Data,"Recent advancements in generative AI and agentic systems aim to develop autonomous research assistants that can accelerate scientific discovery. This paper introduces VER, a framework demonstrating how a Multimodal Large Language Model (MLLM) can function as such an assistant for the complex task of uncovering fundamental governing equations from high-dimensional visual data. Instead of relying on human intuition to define a search space, our autonomous agent leverages the MLLM's pre-trained knowledge and visual perception. We design a series of enhanced visual prompts and a hypothesis-assessment-iteration reasoning chain, enabling the MLLM to autonomously identify intrinsic physical coordinates and propose candidate symbolic terms. This process serves as a case study for AI assistants conducting scientific research by integrating multi-modal information with reasoning capabilities. Quantitative and qualitative evaluations show that our MLLM-based assistant effectively discovers correct physical laws from both simulated and real-world videos, improving long-term prediction accuracy by average 26.96\%. Our work represents a concrete step towards creating AI assistants that are seamlessly integrated into the fabric of scientific discovery.",Ruikun Li; Yan Lu,lrk23@mails.tsinghua.edu.cn; luyan17@mail.ustc.edu.cn,,,
AAAI26_W10_39,Bridging Vision and Language for Generative SEM Analysis in Materials Science,"Understanding the structure is vital for designing advanced materials with tailored properties. With advances of AI methods, such structure can be learned by vision-language model (VLM). Here, we present a vision-language framework -  \textit{VL4SEM} to automate the scanning electron microscopy (SEM) images generation process of double-network hydrogels. Using a curated dataset of PPy-PVA hydrogels with expert-labeled descriptions, our framework leverages few shot learning to align structural semantics between text and image inputs. By integrating large language models with a UNet-based diffusion model through FiLM and LoRA modules, \textit{VL4SEM} enables predictive microstructure inference in text-driven SEM image generation. Each component of our framework is further analyzed theoretically and in simulations. During experiment, \textit{VL4SEM} outperforms ChatGPT-4.0 by generating SEM images in that it more accurately reflects the fibrous, heterogeneous microstructures of real PPy-PVA hydrogels. Compared to LLMs, \textit{VL4SEM} captures physical textures, which is crucial for realistic chemical function analysis. This approach opens a new avenue for inverse design and data-driven discovery in soft materials research.",Xuanzhou Chen; Dong Zhang,xchen920@gatech.edu; dz39@uakron.edu,,,
AAAI26_W10_40,Spectral Graph Theory Conjecture Generation,"This paper deals with the automated generation of mathematical conjectures. More specifically, we address spectral graph theory conjecture generation. We combine the automated generation of conjectures with the automated refutation of the generated conjectures to produce hundreds of thousands of new spectral graph theory conjectures that are difficult to refute.",Elie Duhamel; Tristan Cazenave,elie.duhamel@grenoble-inp.org; tristan.cazenave@dauphine.fr,,Poster 2,WS157
AAAI26_W10_41,ChemLabs on ChemO: A Multi-Agent System for Multimodal Reasoning on IChO 2025,"Olympiad-level benchmarks in mathematics and physics are crucial testbeds for advanced AI reasoning, but chem- istry, with its unique multimodal symbolic language, has remained an open challenge. We introduce ChemO, a new benchmark built from the International Chemistry Olympiad (IChO) 2025. ChemO features two key inno- vations for automated assessment: Assessment-Equivalent Reformulation (AER), which converts problems requiring visual outputs (e.g., drawing molecules) into computation- ally tractable formats, and Structured Visual Enhancement (SVE), a diagnostic mechanism to disentangle a model’s vi- sual perception capabilities from its core chemical reason- ing. To tackle this benchmark, we propose ChemLabs, a hierarchical multi-agent framework that mimics human ex- pert collaboration through specialized agents for problem decomposition, perception, reasoning, and auditing. Exper- iments on state-of-the-art multimodal models demonstrate that combining SVE with our multi-agent system yields dra- matic performance gains. Our top configuration achieves a score of 93.6 out of 100, surpassing an estimated hu- man gold medal threshold and establishing a new state- of-the-art in automated chemical problem-solving.","Qiang Xu, Shengyuan Bai, Leqing Chen, Zijing Liu, Qiang Xu",liyu@idea.edu.cn,,Poster 1,WS156
